---
title: "ProjectCARE: Suicide Prevention from Tissue Damage Images" 
excerpt: "<div><p><img style='object-fit:scale-down; display: block; float: left; padding-right: 0.5em; padding-bottom: 0.5em; max-height: 150px;' src='/images/scar-detection.png' width='300' height='150'></p>An NIH-funded clinical decision-support tool to automatically to locate and characterize non-suicidal self-injury tissue damage in images for suicide risk prediction, with the potential to lower suicide rates using custom objective functions and training methods to learn from our noisy, real-world data.<br><em>In collaboration with Harvard University and Massachusetts General Hospital.</em></div>"
collection: portfolio
---
<img style='object-fit:scale-down; display: block; float: left; padding-right: 0.5em; padding-bottom: 0.5em; max-height: 300px;' src='/images/scar-detection.png' width='100%' height='300'>
<br>
<style>
    .double-val-label {
	 /* display: table; */
	 font-family: 'Roboto', sans-serif;
	 margin: 0.4em auto;
     display: inline-block;
     
}
 .double-val-label>span {
	 background-color: #656565;
	 color: #ffffff;
	 display: table-cell;
	 font-size: 0.9em;
	 font-weight: 400;
	 line-height: 1;
	 padding: .3em .6em;
	 text-align: center;
	 vertical-align: baseline;
	 white-space: nowrap;
}
 .double-val-label>span:first-child {
	 border-radius: 0.25em;
}
 .double-val-label>span:nth-child(2) {
	 border-radius: .25em;
}
 .double-val-label>span.primary {
	 background-color: #337ab7;
}
 .double-val-label>span.success {
	 background-color: #5cb85c;
}
 .double-val-label>span.info {
	 background-color: #5bc0de;
}
 .double-val-label>span.warning {
	 background-color: #f0ad4e;
}
 .double-val-label>span.danger {
	 background-color: #d9534f;

 }
 .slideshow-container {
    max-width: 1000px;
    position: relative;
    margin: auto
}

.mySlides {
    display: none;
  height: 400px;
  border: solid 1px black;
     
}

.prev,
.next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    width: auto;
    margin-top: -22px;
    padding: 16px;
    color: #222428;
    font-weight: bold;
    font-size: 30px;
    transition: .6s ease;
    border-radius: 0 3px 3px 0
}

.next {
    right: -50px;
    border-radius: 3px 3px 3px 3px
}

.prev {
    left: -50px;
    border-radius: 3px 3px 3px 3px
}

.prev:hover,
.next:hover {
    color: #f2f2f2;
    background-color: rgba(0, 0, 0, 0.8)
}

.text {
    color: #f2f2f2;
    font-size: 15px;
    padding-top: 12px;
  padding-bottom: 12px;
    position: absolute;
    bottom: 0;
    width: 100%;
    text-align: center;
    background-color: #222428
}

.numbertext {
    color: #f2f2f2;
    font-size: 12px;
    padding: 8px 12px;
    position: absolute;
    top: 0
}

.dot {
    cursor: pointer;
    height: 15px;
    width: 15px;
    margin: 0 2px;
    background-color: #bbb;
    border-radius: 50%;
    display: inline-block;
    transition: background-color .6s ease
}

.active,
.dot:hover {
    background-color: #717171
}
 
</style>
<div class="double-val-label"><span class="success">Artificial Intelligence</span></div>
<div class="double-val-label"><span class="primary">Computer Vision</span></div>
<div class="double-val-label"><span class="info">AI4SocialGood</span></div>
<br>
An NIH-funded clinical decision-support tool to automate the assessment of tissue damage in self-injury images for suicide risk prediction, with the potential to lower suicide rates. Visual assessment of tissue damage is a high-risk setting that demands robust and reliable CV models. The project resulted in a reliable suicide-risk metric and better tissue damage detection (mAP=0.81) compared to trained human annotators (mAP=0.64). A manuscript on this project is being prepared, and we are currently developing an NIH-R01 application to integrate this tool into electronic health records.
<em>In collaboration with Harvard University and Massachusetts General Hospital.</em>
<br>
<br>
<strong>Technologies Used:</strong> OpenMMLab, RNNs, Pytorch, opencv.
<br>
<br>
<strong>My Role:</strong> 
<ul>
<li>Designed multiple components for the REDCap survey to collect participant data.</li>
<li>Web-scraped 9000 images of human limbs from Google Images, Flickr, and Shutterstock. Manually verified all 9000 images.</li>
<li>Trained a two-step detector with modified loss for FasterRCNN and Co-DETR.</li>
<li>Built end-to-end data processing pipelines for all the participant data of 30k images.</li>
<li>Models for 84% accuracy with classifiers on the scars classifying method, type, etc. using self-supervised learning.</li>
<li>Used a two-step detector to harvest false positives to tune the detector.</li>
</ul>
<div style="text-align: center;"><h3>Project Gallery</h3></div>
<meta name="viewport" content="width=device-width, initial-scale=1">

<div class="slideshow-container" style="width: 600px; height: 400px;">
    <div class="slideshow-inner">
      <div class="mySlides fade">
        <img  src='/images/nssi.png' style='width: 100%; max-width: 600px; max-height: 400px;' alt="sally lightfoot crab"/>
        <div class="text">Scar Detector</div>
      </div>
      <div class="mySlides fade">
        <img  src='/images/participant_accuracy.png' style='width: 100%; max-width: 600px; max-height: 400px;' alt="fighting nazca boobies"/>
        <div class="text">No. of Participants vs. Accuracy</div>
      </div>
    </div>
    <a class="prev" onclick='plusSlides(-1)'>&#10094;</a>
    <a class="next" onclick='plusSlides(1)'>&#10095;</a>
</div>
<br/>
    
<div style='text-align: center;'>
    <span class="dot" onclick='currentSlide(1)'></span>
    <span class="dot" onclick='currentSlide(2)'></span>
</div>
<script defer src="/assets/js/portfolio/11real-time-action-recognition.js"></script>